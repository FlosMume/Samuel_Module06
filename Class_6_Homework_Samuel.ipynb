{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Assignment: Extending the Voice Agent with Function Calling.\n",
    "This week, we dive into a crucial capability of modern AI agents: function calling. Real-world agents are not limited to generating text‚Äîthey interact with tools, run computations, query databases, and more. In this assignment, you will extend the multi-turn voice agent you built in Week 3 with the ability to automatically execute tools based on natural language commands.\n",
    "\n",
    "Using Llama 3 as your core LLM, you‚Äôll teach the model to recognize when a user wants to search arXiv papers or perform a math calculation, and respond by outputting a structured function call. Your agent will then parse that function call, execute the appropriate tool, and return the result to the user via text-to-speech. This is a major step toward building a fully autonomous research assistant that can act on intent‚Äînot just reply with facts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "* **Function Calling with LLMs:** Learn how to use function/tool calling by prompting Llama¬†3 to output structured calls (e.g., JSON) for external functions.\n",
    "* **Intent Parsing and Tool Mapping:** Practice parsing user queries to determine the intent (e.g., search a database or perform a calculation) and mapping that intent to specific tool functions like `search_arxiv(query)` and `calculate(expression)`.\n",
    "* **Integrating Tools into the Agent:** Extend the Week¬†3 multi-turn voice agent pipeline (ASR ‚Üí LLM ‚Üí TTS) so that the LLM can trigger code execution. The agent should automatically call the right function based on the LLM‚Äôs output, and then speak the returned result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Project Design\n",
    "\n",
    "Reuse the voice-chat pipeline from Week¬†3 and enhance the LLM step to support calling external tools. The key tasks include:\n",
    "\n",
    "* **Tool Functions:** Implement two helper functions:\n",
    "\n",
    "\n",
    "  * `search_arxiv(query: str) -> str`: Simulates or performs an arXiv search and returns a relevant passage or summary for the query.\n",
    "  * `calculate(expression: str) -> str`: Evaluates a mathematical expression (using `sympy` or `eval`) and returns the result as text.\n",
    "\n",
    "* **Prompt Engineering:** Modify the Llama¬†3 system/user prompts so the model knows to generate structured function-call outputs when appropriate. For example, instruct it that if the user‚Äôs question can be answered by searching arXiv or doing math, it should output a JSON-like call, such as:\n",
    "\n",
    "  ```json\n",
    "  {\"function\": \"calculate\", \"arguments\": {\"expression\": \"2+2\"}}\n",
    "  ```\n",
    "\n",
    "  or\n",
    "\n",
    "  ```json\n",
    "  {\"function\": \"search_arxiv\", \"arguments\": {\"query\": \"quantum entanglement\"}}\n",
    "  ```\n",
    "\n",
    "  Otherwise it should respond normally in text.\n",
    "\n",
    "* **Detecting and Calling Tools:** After the LLM generates a response, check if it is a function call. Parse the JSON output from the LLM to extract the function name and arguments. If it is a call, invoke the corresponding Python function (`search_arxiv` or `calculate`) with those arguments and capture its result. Use this result as the assistant‚Äôs reply (to be spoken by TTS). If the LLM output is normal text, just use it as the assistant‚Äôs response without calling any function.\n",
    "\n",
    "* **Fallback Behavior:** Ensure the voice agent handles all cases. If the LLM‚Äôs output cannot be parsed as a function call (or if the named function is unknown), fall back to replying with a standard text response or an error message as appropriate.\n",
    "\n",
    "\n",
    "## Setting\n",
    "\n",
    "### Create and activate virtual environment\n",
    "\n",
    "\n",
    "(base) C:\\Users\\ch939>cd C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6\n",
    "\n",
    "(base) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda create -n vx_venv python=3.10 -y\n",
    "\n",
    "(base) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda activate vx_venv\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6\\bitsandbytes-windows>pip install bitsandbytes-windows\n",
    "\n",
    "### Install required packages\n",
    "\n",
    "\n",
    "(mod6venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install pip -y\n",
    "\n",
    "(mod6venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "(mod6venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install python-dotenv\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install -c conda-forge transformers sentencepiece\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install mkl=2021.4.0\n",
    "\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install accelerate\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda config --add channels conda-forge\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda config --set channel_priority strict\n",
    "\n",
    "\n",
    "Math\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install -c conda-forge sympy\n",
    "\n",
    "APIs\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install -c conda-forge huggingface_hub\n",
    "\n",
    "(mod6venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install anthropic\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install openai\n",
    "\n",
    "FastAPI\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install -c conda-forge fastapi uvicorn python-multipart\n",
    "\n",
    "\n",
    "Audio\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install -c conda-forge pydub\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install SpeechRecognition pyttsx3\n",
    "\n",
    "Optional\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install arxiv\n",
    "\n",
    "\n",
    "After switch to model_id = \"mistralai/Mistral-7B-Instruct-v0.3\" and \"alokabhishek/Meta-Llama-3-8B-Instruct-bnb-4bit\"\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6\\bitsandbytes-windows>pip install protobuf\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install -c conda-forge ipywidgets\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda config --set channel_priority strict\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install --upgrade bitsandbytes\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip show bitsandbytes\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install arxiv\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install pyaudio\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>conda install -c conda-forge protobuf\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install SpeechRecognition\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install pyttsx3\n",
    "\n",
    "(vx_venv) C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6>pip install fastapi uvicorn\n",
    "\n",
    "My system‚ÄîWindows 11, RTX 4070 SUPER‚Äîoffers solid GPU support and abundant VRAM (~12 GB).\n",
    "\n",
    "\n",
    "## Starter Code\n",
    "\n",
    "We provide snippets to help you get started:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: GPU & CUDA Availability in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Version: 11.8\n",
      "GPU Count: 1\n",
      "Current Device: 0\n",
      "Device Name: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"GPU Count:\", torch.cuda.device_count())\n",
    "print(\"Current Device:\", torch.cuda.current_device())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è OpenAI not installed. Run: pip install openai\n",
      "‚ö†Ô∏è Anthropic not installed. Run: pip install anthropic\n",
      "üîë Hugging Face Token: hf_OI...\n",
      "üîë OpenAI API Key: sk-pr...\n",
      "\n",
      "üìã System Information:\n",
      "‚úÖ Python Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:42:04) [MSC v.1943 64 bit (AMD64)]\n",
      "‚úÖ PyTorch Version: 2.5.1\n",
      "‚úÖ CUDA Available: True\n",
      "‚úÖ GPU Device: NVIDIA GeForce RTX 4070 SUPER\n",
      "\n",
      "üéâ All dependencies and systems are ready!\n"
     ]
    }
   ],
   "source": [
    "# API clients\n",
    "try:\n",
    "    import openai\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "    print(\"‚úÖ OpenAI available\")\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è OpenAI not installed. Run: pip install openai\")\n",
    "\n",
    "try:\n",
    "    import anthropic\n",
    "    ANTHROPIC_AVAILABLE = True\n",
    "    print(\"‚úÖ Anthropic available\")\n",
    "except ImportError:\n",
    "    ANTHROPIC_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Anthropic not installed. Run: pip install anthropic\")\n",
    "\n",
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(r\"C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6\\.env\")\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "temp = hf_token\n",
    "\n",
    "print(\"üîë Hugging Face Token:\", (hf_token[:5] + \"...\") if hf_token else \"Not found\")\n",
    "print(\"üîë OpenAI API Key:\", (openai_key[:5] + \"...\") if openai_key else \"Not found\")\n",
    "\n",
    "\n",
    "# üí° System & Hardware Info\n",
    "import sys\n",
    "import torch\n",
    "print(\"\\nüìã System Information:\")\n",
    "print(f\"‚úÖ Python Version: {sys.version}\")\n",
    "print(f\"‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚úÖ GPU Device: No GPU detected\")\n",
    "\n",
    "# ‚úÖ Final Confirmation\n",
    "print(\"\\nüéâ All dependencies and systems are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: Can You Load Llama 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your request to access this repository has been submitted and is awaiting a review from the repository authors. \n",
    "istly, Need the approval here https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec74a49795240998465f84c36606dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "login(token = os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "# model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "# model_id = \"TheBloke/OpenHermes-2-Mistral-7B-GPTQ\"\n",
    "# model_id = \"Qwen/Qwen1.5-7B-Chat-AWQ\"\n",
    "\n",
    "\n",
    "# Step 1: Define quantization config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # or load_in_8bit=True\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # adjust as needed\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Load model and tokenizer with quantization config\n",
    "model_id = \"alokabhishek/Meta-Llama-3-8B-Instruct-bnb-4bit\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token = temp)\n",
    "\n",
    "\n",
    "# Step 3: Create pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week6/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ vx_venv/                  # virtual env\n",
    "‚îú‚îÄ‚îÄ voice_agent.py            # main agent logic\n",
    "‚îú‚îÄ‚îÄ tools.py                  # search_arxiv, calculate\n",
    "‚îú‚îÄ‚îÄ prompts.py                # system prompt with function instructions\n",
    "‚îú‚îÄ‚îÄ .env                      # HF_TOKEN, OPENAI_API_KEY\n",
    "‚îî‚îÄ‚îÄ test_logs.txt             # sample logs (deliverable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools.py\n",
    "import arxiv\n",
    "from sympy import sympify\n",
    "from typing import Dict, Any\n",
    "\n",
    "def search_arxiv(query: str) -> str:\n",
    "    \"\"\"Search arXiv and return paper titles or abstract snippets.\"\"\"\n",
    "    try:\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=1,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        result = next(search.results(), None)\n",
    "        if result:\n",
    "            return f\"Top result: '{result.title}'. Abstract: {result.summary[:200]}...\"\n",
    "        else:\n",
    "            return f\"No papers found for '{query}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error searching arXiv: {e}\"\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Safely evaluate mathematical expressions.\"\"\"\n",
    "    try:\n",
    "        result = sympify(expression)\n",
    "        return str(result.evalf()) if result.is_number else str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prompts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts.py\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful AI research assistant with access to tools. \n",
    "If the user asks to search for academic papers, respond with a JSON function call:\n",
    "{\"function\": \"search_arxiv\", \"arguments\": {\"query\": \"your query\"}}\n",
    "\n",
    "If the user asks to calculate a math expression, respond with:\n",
    "{\"function\": \"calculate\", \"arguments\": {\"expression\": \"2+2\"}}\n",
    "\n",
    "Only output JSON when calling a function. Otherwise, respond naturally in plain text.\n",
    "Do not add any extra text before or after the JSON.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### voice_agent.py  using meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st option : meta-llama/Meta-Llama-3-7B-Instruct is not available, meta-llama/Meta-Llama-3-8B-Instruct have to be used, it will be slow with offload, allow some layers to run on CPU with fp32 offload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warn] HF_TOKEN is not set. Set it in Week5/.env to access Meta models.\n",
      "\n",
      "[Init] Loading meta-llama/Meta-Llama-3-8B-Instruct with 8-bit + CPU fp32 offload‚Ä¶\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fdc4ef31114ae9af563afe0b951dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test] Arithmetic via tool call:\n",
      "11.0000000000000\n",
      "\n",
      "[Test] ArXiv search via tool call:\n",
      "Error: bad arguments for 'search_arxiv': search_arxiv() got an unexpected keyword argument 'max_results'\n",
      "\n",
      "[Test] Small talk (no tools):\n",
      "I'm doing well, thanks for asking! I'm a helpful AI research assistant, here to assist you with your queries. What can I help you with today?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Environment & constants\n",
    "# =========================\n",
    "WORKDIR = r\"C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6\"\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv(r\"C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6\\.env\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    print(\"[Warn] HF_TOKEN is not set. Set it in Week6/.env to access Meta models.\")\n",
    "\n",
    "# =========================\n",
    "# Model selection (Llama 3 8B with CPU fp32 offload)\n",
    "# =========================\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Quantization/offload:\n",
    "# - 8-bit weights to reduce VRAM pressure\n",
    "# - enable fp32 CPU offload for layers that don't fit in VRAM\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "# Optional CUDA configuration tweaks\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# =========================\n",
    "# Load model & tokenizer\n",
    "# =========================\n",
    "print(f\"\\n[Init] Loading {MODEL_ID} with 8-bit + CPU fp32 offload‚Ä¶\")\n",
    "\n",
    "# Tokenizer\n",
    "# Note: Access to Meta Llama models on HF requires accepting the license\n",
    "# and setting HF_TOKEN in your environment.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "# Ensure pad token is set (Llama uses eos as pad)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Model with automatic layer placement and CPU offload fallback\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",                # let Accelerate split across GPU/CPU\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Text-generation pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,   # only return the completion\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Tools & prompting\n",
    "# =========================\n",
    "from tools import search_arxiv, calculate\n",
    "from prompts import SYSTEM_PROMPT\n",
    "\n",
    "TOOLS = {\n",
    "    \"search_arxiv\": search_arxiv,\n",
    "    \"calculate\": calculate,\n",
    "}\n",
    "\n",
    "TOOL_SPEC = (\n",
    "    \"You have access to tools. If and only if a tool is needed, output a single JSON object on one line with keys 'function' and 'arguments'. \"\n",
    "    \"The value of 'function' must be one of: 'search_arxiv', 'calculate'. \"\n",
    "    \"The value of 'arguments' must be a JSON object with the call parameters. \"\n",
    "    \"Do not add other keys. Do not add prose before or after the JSON.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_prompt(user_text: str) -> str:\n",
    "    \"\"\"Build a chat-formatted prompt for Llama 3 with a tool-use contract.\"\"\"\n",
    "    system_msg = f\"{SYSTEM_PROMPT}\\n\\n{TOOL_SPEC}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "_JSON_BLOCK_RE = re.compile(r\"```(?:json)?\\s*(\\{[\\s\\S]*?\\})\\s*```\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def _extract_first_json(text: str) -> str | None:\n",
    "    \"\"\"Return the first JSON object found in text, supporting fenced blocks.\"\"\"\n",
    "    m = _JSON_BLOCK_RE.search(text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    # Fallback: naive brace matching of the first {...}\n",
    "    brace_stack = 0\n",
    "    start = None\n",
    "    for i, ch in enumerate(text):\n",
    "        if ch == '{':\n",
    "            if brace_stack == 0:\n",
    "                start = i\n",
    "            brace_stack += 1\n",
    "        elif ch == '}':\n",
    "            if brace_stack > 0:\n",
    "                brace_stack -= 1\n",
    "                if brace_stack == 0 and start is not None:\n",
    "                    return text[start:i+1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def route_llm_output(llm_output: str) -> str:\n",
    "    \"\"\"Parse potential tool call and route to Python functions.\"\"\"\n",
    "    llm_output = llm_output.strip()\n",
    "    candidate = _extract_first_json(llm_output) or llm_output\n",
    "\n",
    "    try:\n",
    "        call = json.loads(candidate)\n",
    "        func_name = call.get(\"function\")\n",
    "        args = call.get(\"arguments\", {})\n",
    "\n",
    "        if func_name in TOOLS:\n",
    "            try:\n",
    "                result = TOOLS[func_name](**args)\n",
    "            except TypeError as e:\n",
    "                return f\"Error: bad arguments for '{func_name}': {e}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error while executing '{func_name}': {e}\"\n",
    "            return result if isinstance(result, str) else json.dumps(result, ensure_ascii=False)\n",
    "        else:\n",
    "            return llm_output\n",
    "    except json.JSONDecodeError:\n",
    "        return llm_output\n",
    "\n",
    "\n",
    "def agent_query(\n",
    "    user_text: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    \"\"\"Main agent loop with tool-routing and CPU offload-friendly generation.\"\"\"\n",
    "    prompt = build_prompt(user_text)\n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    assistant_reply = outputs[0][\"generated_text\"].strip()\n",
    "    final = route_llm_output(assistant_reply)\n",
    "    return final\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n[Test] Arithmetic via tool call:\")\n",
    "    print(agent_query(\"What is 5 + 3 * 2? Use the calculator tool if available.\"))\n",
    "\n",
    "    print(\"\\n[Test] ArXiv search via tool call:\")\n",
    "    print(agent_query(\"Search arXiv for recent papers on quantum entanglement, last 2 years, return 3 items.\"))\n",
    "\n",
    "    print(\"\\n[Test] Small talk (no tools):\")\n",
    "    print(agent_query(\"Hello, how are you?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voice_agent: The 2nd option using mistralai/Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voice_agent using model: mistralai/Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Init] Loading mistralai/Mistral-7B-Instruct-v0.3 in 4-bit‚Ä¶\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112d3744a9ce4f449045f3695c1a5a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ch939\\anaconda3\\envs\\vx_venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ch939\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85404b31092f4df881c763b6d8cce666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db78243bea5147c3a36e16d51e8e2a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f57c052cd641a9b22150e6251b7212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1d20f3a52c466da1de487c6b5bddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171ef9f763204ef298a333be986cb8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eaeb6ac2dc346a59911812f1f6f59e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test] Arithmetic via tool call:\n",
      "11.0000000000000\n",
      "\n",
      "[Test] ArXiv search via tool call:\n",
      "Error: bad arguments for 'search_arxiv': search_arxiv() got an unexpected keyword argument 'year_start'\n",
      "\n",
      "[Test] Small talk (no tools):\n",
      "Top result: 'TrackMeNot-so-good-after-all'. Abstract: TrackMeNot is a Firefox plugin with laudable intentions - protecting your\n",
      "privacy. By issuing a customizable stream of random search queries on its\n",
      "users' behalf, TrackMeNot surmises that enough searc...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Environment & constants\n",
    "# =========================\n",
    "# Working dir (for clarity in logs or future file ops)\n",
    "WORKDIR = r\"C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6\"\n",
    "\n",
    "# Load API keys from Week5 .env as provided\n",
    "load_dotenv(r\"C:\\Users\\ch939\\Downloads\\LLMBootCampCodes\\Week6\\.env\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# =========================\n",
    "# Model selection (fits 12GB VRAM smoothly)\n",
    "# =========================\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Quantization: 4-bit for headroom on RTX 4070 SUPER (12 GB)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Load model & tokenizer\n",
    "# =========================\n",
    "print(f\"\\n[Init] Loading {MODEL_ID} in 4-bit‚Ä¶\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Text-generation pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,   # only return the completion\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Tools & prompting\n",
    "# =========================\n",
    "from tools import search_arxiv, calculate\n",
    "from prompts import SYSTEM_PROMPT\n",
    "\n",
    "TOOLS = {\n",
    "    \"search_arxiv\": search_arxiv,\n",
    "    \"calculate\": calculate,\n",
    "}\n",
    "\n",
    "# A compact, explicit tool contract the model can follow\n",
    "TOOL_SPEC = (\n",
    "    \"You have access to tools. If and only if a tool is needed, output a single JSON object on one line with keys 'function' and 'arguments'. \"\n",
    "    \"The value of 'function' must be one of: 'search_arxiv', 'calculate'. \"\n",
    "    \"The value of 'arguments' must be a JSON object with the call parameters. \"\n",
    "    \"Do not add other keys. Do not add prose before or after the JSON.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_prompt(user_text: str) -> str:\n",
    "    \"\"\"Use the chat template for Mistral-Instruct.\n",
    "    System includes function-calling contract and the user's system prompt.\n",
    "    \"\"\"\n",
    "    system_msg = f\"{SYSTEM_PROMPT}\\n\\n{TOOL_SPEC}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "_JSON_BLOCK_RE = re.compile(r\"```(?:json)?\\s*(\\{[\\s\\S]*?\\})\\s*```\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def _extract_first_json(text: str) -> str | None:\n",
    "    \"\"\"Return the first JSON object found in text, supporting fenced blocks.\n",
    "    If none found, return None.\n",
    "    \"\"\"\n",
    "    # Try fenced JSON first\n",
    "    m = _JSON_BLOCK_RE.search(text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    # Fallback: naive brace matching of the first {...}\n",
    "    brace_stack = 0\n",
    "    start = None\n",
    "    for i, ch in enumerate(text):\n",
    "        if ch == '{':\n",
    "            if brace_stack == 0:\n",
    "                start = i\n",
    "            brace_stack += 1\n",
    "        elif ch == '}':\n",
    "            if brace_stack > 0:\n",
    "                brace_stack -= 1\n",
    "                if brace_stack == 0 and start is not None:\n",
    "                    return text[start:i+1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def route_llm_output(llm_output: str) -> str:\n",
    "    \"\"\"Parse potential tool call and route to Python functions.\n",
    "    - Expects a single-line JSON object like {\"function\": \"name\", \"arguments\": {...}}\n",
    "    - If not JSON, return the original text.\n",
    "    \"\"\"\n",
    "    llm_output = llm_output.strip()\n",
    "\n",
    "    candidate = _extract_first_json(llm_output) or llm_output\n",
    "\n",
    "    try:\n",
    "        call = json.loads(candidate)\n",
    "        func_name = call.get(\"function\")\n",
    "        args = call.get(\"arguments\", {})\n",
    "\n",
    "        if func_name in TOOLS:\n",
    "            try:\n",
    "                result = TOOLS[func_name](**args)\n",
    "            except TypeError as e:\n",
    "                return f\"Error: bad arguments for '{func_name}': {e}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error while executing '{func_name}': {e}\"\n",
    "            return result if isinstance(result, str) else json.dumps(result, ensure_ascii=False)\n",
    "        else:\n",
    "            # Not a known function ‚Äî just return original assistant text\n",
    "            return llm_output\n",
    "    except json.JSONDecodeError:\n",
    "        # Not JSON ‚Äî treat as a normal assistant reply\n",
    "        return llm_output\n",
    "\n",
    "\n",
    "def agent_query(user_text: str, max_new_tokens: int = 256, temperature: float = 0.6, top_p: float = 0.9) -> str:\n",
    "    \"\"\"Main agent loop.\n",
    "    - Builds a structured chat prompt with a strict tool-calling contract.\n",
    "    - Generates a response with Mistral-7B-Instruct.\n",
    "    - Routes tool calls if present; otherwise returns model text.\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(user_text)\n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    assistant_reply = outputs[0][\"generated_text\"].strip()\n",
    "\n",
    "    # Try tool routing\n",
    "    final = route_llm_output(assistant_reply)\n",
    "    return final\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Quick smoke tests\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n[Test] Arithmetic via tool call:\")\n",
    "    print(agent_query(\"What is 5 + 3 * 2? Use the calculator tool if available.\"))\n",
    "\n",
    "    print(\"\\n[Test] ArXiv search via tool call:\")\n",
    "    print(agent_query(\"Search arXiv for recent papers on quantum entanglement, last 2 years, return 3 items.\"))\n",
    "\n",
    "    print(\"\\n[Test] Small talk (no tools):\")\n",
    "    print(agent_query(\"Hello, how are you?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-use your Week 3 ASR/TTS pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "def listen():\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = r.listen(source)\n",
    "        try:\n",
    "            return r.recognize_google(audio)\n",
    "        except:\n",
    "            return \"Sorry, I didn't catch that.\"\n",
    "\n",
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then intege"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n"
     ]
    }
   ],
   "source": [
    "query = listen()\n",
    "response = agent_query(query)\n",
    "speak(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function tool stubs (starter implementations)\n",
    "def search_arxiv(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulate an arXiv search or return a dummy passage for the given query.\n",
    "    In a real system, this might query the arXiv API and extract a summary.\n",
    "    \"\"\"\n",
    "    # Example placeholder implementation:\n",
    "    return f\"[arXiv snippet related to '{query}']\"\n",
    "\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate a mathematical expression and return the result as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sympy import sympify\n",
    "        result = sympify(expression)  # use sympy for safe evaluation\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialogue engine: function-routing logic\n",
    "import json\n",
    "\n",
    "def route_llm_output(llm_output: str) -> str:\n",
    "    \"\"\"\n",
    "    Route LLM response to the correct tool if it's a function call, else return the text.\n",
    "    Expects LLM output in JSON format like {'function': ..., 'arguments': {...}}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = json.loads(llm_output)\n",
    "        func_name = output.get(\"function\")\n",
    "        args = output.get(\"arguments\", {})\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Not a JSON function call; return the text directly\n",
    "        return llm_output\n",
    "\n",
    "    if func_name == \"search_arxiv\":\n",
    "        query = args.get(\"query\", \"\")\n",
    "        return search_arxiv(query)\n",
    "    elif func_name == \"calculate\":\n",
    "        expr = args.get(\"expression\", \"\")\n",
    "        return calculate(expr)\n",
    "    else:\n",
    "        return f\"Error: Unknown function '{func_name}'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example FastAPI endpoint (sketch)\n",
    "from fastapi import FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/api/voice-query/\")\n",
    "async def voice_query_endpoint(request: dict):\n",
    "    # Assume request has 'text': the user's query string\n",
    "    user_text = request.get(\"text\", \"\")\n",
    "    # Call Llama 3 model (instructed to output function calls when needed)\n",
    "    llm_response = llama3_chat_model(user_text)\n",
    "    # Process LLM output and possibly call tools\n",
    "    reply_text = route_llm_output(llm_response)\n",
    "    # Convert reply_text to speech (TTS) and return it\n",
    "    return {\"response\": reply_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above code outlines where to plug in your LLM call and audio I/O. Integrate the function-calling logic into your existing voice agent framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables\n",
    "\n",
    "* **Codebase:** Submit your updated voice agent code with function calling integration. Document any new modules or changes clearly.\n",
    "* **Test Logs:** Provide sample logs for at least three queries, showing:\n",
    "\n",
    "  1. The user‚Äôs query text.\n",
    "  2. The raw LLM response (JSON function call or normal text).\n",
    "  3. Any function call made and its output.\n",
    "  4. The final assistant response.\n",
    "* **Demo Video:** A 1‚Äì2 minute demo of the voice agent. Show the agent handling:\n",
    "\n",
    "  * A math query (invoking `calculate`).\n",
    "  * An arXiv search query (invoking `search_arxiv`).\n",
    "  * A normal query (no function call).\n",
    "\n",
    "## Exploration Tips\n",
    "\n",
    "* **Extend Tools:** Try adding new tools (e.g. a weather lookup or translation). Define their function signatures and integrate them into your agent.\n",
    "* **Tool Registry:** Create a dictionary or registry of function names to callables to simplify routing logic when you have multiple tools.\n",
    "* **Other LLMs:** Experiment with other models that support function calling (e.g. GPT-4 with its function calling API). Compare how their output format and reliability differ from Llama¬†3.\n",
    "* **Error Handling:** Make sure your agent handles invalid inputs gracefully (e.g. a malformed math expression should not crash the agent).\n",
    "* **Chained Calls (Advanced):** As a challenge, allow the agent to use one tool‚Äôs output as context for another. For example, it could `search_arxiv` for a value and then `calculate` something with that value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
